{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Uo1XqpZY8aPKaTuO-1haQ6FU0zmmdg5z","timestamp":1744483577829},{"file_id":"1M_vAqGRyPk5IWX2HlDxlmhu3rBJ8KTP_","timestamp":1744477963054}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FBlCOykHIUhi","executionInfo":{"status":"ok","timestamp":1744483200672,"user_tz":240,"elapsed":2608,"user":{"displayName":"王Fanmei","userId":"11430698902803120911"}},"outputId":"b25f3611-fbf5-4cd5-e4b5-3db3b5df06e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unidecode\n","  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n","Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.8\n"]}],"source":["!pip install unidecode openpyxl"]},{"cell_type":"code","source":["import os\n","import re\n","import glob\n","import html\n","import pandas as pd\n","from unidecode import unidecode\n","\n","FOLDER_PATH = \"/content/extracted_RS_raw_data\"\n","FILE_PATTERN = \"*.csv\"\n","OUTPUT_CSV = \"/content/cleaned_merged_data.csv\"\n","\n","TITLE_COL = \"title\"\n","SELFTEXT_COL = \"selftext\"\n","CREATED_UTC_COL = \"created_utc\"\n","\n","# 1. Text cleaning\n","def clean_text(text: str) -> str:\n","    if not isinstance(text, str):\n","        return \"\"\n","    text_stripped = text.strip().lower()\n","    if text_stripped in [\"[deleted]\", \"[removed]\", \"\"]:\n","        return \"\"\n","    text = html.unescape(text)\n","    text = unidecode(text)\n","    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n","    text = re.sub(r\"[^a-zA-Z0-9\\s\\.\\,\\!\\?\\']\", \" \", text)\n","    text = text.lower()\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","# 2. Read and merge files\n","def read_and_merge(folder_path: str, file_pattern: str) -> pd.DataFrame:\n","    files = sorted(glob.glob(os.path.join(folder_path, file_pattern)))\n","    if not files:\n","        print(f\"[WARNING] No files found in: {folder_path}/{file_pattern}\")\n","        return pd.DataFrame()\n","\n","    all_dfs = []\n","    for file_path in files:\n","        print(f\"[INFO] Reading: {file_path}\")\n","        if file_path.endswith(\".csv\"):\n","            df = pd.read_csv(file_path, encoding=\"utf-8\", keep_default_na=False)\n","        else:\n","            df = pd.read_excel(file_path, engine=\"openpyxl\", keep_default_na=False)\n","        all_dfs.append(df)\n","\n","    if not all_dfs:\n","        return pd.DataFrame()\n","\n","    merged_df = pd.concat(all_dfs, ignore_index=True)\n","    print(f\"[INFO] Successfully merged {len(files)} file(s). Total rows: {len(merged_df)}\")\n","    return merged_df\n","\n","# 3. Main process: merge, clean, save\n","df_merged = read_and_merge(FOLDER_PATH, FILE_PATTERN)\n","\n","if df_merged.empty:\n","    print(\"[ERROR] No data to process. Exiting.\")\n","else:\n","    # a) Add date columns\n","    if CREATED_UTC_COL in df_merged.columns:\n","        df_merged[\"created_dt\"] = pd.to_datetime(df_merged[CREATED_UTC_COL], unit=\"s\", errors=\"coerce\")\n","        df_merged[\"month\"] = df_merged[\"created_dt\"].dt.strftime(\"%Y-%m\")\n","    else:\n","        print(f\"[WARNING] Column '{CREATED_UTC_COL}' not found. Unable to create 'month'.\")\n","        df_merged[\"created_dt\"] = None\n","        df_merged[\"month\"] = None\n","\n","    # b) Remove rows where both title and selftext are useless\n","    def is_useless(text):\n","        return str(text).strip().lower() in [\"[removed]\", \"[deleted]\", \"\"]\n","\n","    df_merged = df_merged[\n","        ~(\n","            df_merged[TITLE_COL].apply(is_useless) &\n","            df_merged[SELFTEXT_COL].apply(is_useless)\n","        )\n","    ].reset_index(drop=True)\n","\n","    # ✅ NEW: Clean [deleted]/[removed] BEFORE merging\n","    def clean_raw_field(text):\n","        if str(text).strip().lower() in [\"[deleted]\", \"[removed]\"]:\n","            return \"\"\n","        return text\n","\n","    # c) Combine + clean\n","    df_merged[\"combined_text\"] = (\n","        df_merged[TITLE_COL].apply(clean_raw_field).fillna(\"\") + \" \" +\n","        df_merged[SELFTEXT_COL].apply(clean_raw_field).fillna(\"\")\n","    )\n","\n","    df_merged[\"clean_text\"] = df_merged[\"combined_text\"].apply(clean_text)\n","\n","    # d) Drop rows with empty clean_text\n","    before_drop = len(df_merged)\n","    df_merged = df_merged[df_merged[\"clean_text\"].str.strip() != \"\"]\n","    after_drop = len(df_merged)\n","    print(f\"[INFO] Dropped {before_drop - after_drop} empty rows. Remaining rows: {after_drop}\")\n","\n","    # e) Save cleaned file\n","    output_path = os.path.join(FOLDER_PATH, OUTPUT_CSV)\n","    df_merged.to_csv(output_path, index=False, encoding=\"utf-8\")\n","    print(f\"[DONE] Cleaned data saved to: {output_path}\")\n","\n","    print(\"[INFO] DataFrame columns:\", df_merged.columns.tolist())\n","    print(\"[INFO] Head of the DataFrame:\")\n","    print(df_merged.head(3))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FMNLbgcUTJhk","executionInfo":{"status":"ok","timestamp":1744483510486,"user_tz":240,"elapsed":4239,"user":{"displayName":"王Fanmei","userId":"11430698902803120911"}},"outputId":"74b7aa50-edf1-40b9-f35b-b0b5950fac4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Reading: /content/extracted_RS_raw_data/extracted_RS_2022-12_filtered.csv\n","[INFO] Reading: /content/extracted_RS_raw_data/extracted_RS_2023-01_filtered.csv\n","[INFO] Reading: /content/extracted_RS_raw_data/extracted_RS_2023-02_filtered.csv\n","[INFO] Reading: /content/extracted_RS_raw_data/extracted_RS_2023-03_filtered.csv\n","[INFO] Reading: /content/extracted_RS_raw_data/extracted_RS_2023-04_filtered.csv\n","[INFO] Reading: /content/extracted_RS_raw_data/extracted_RS_2023-05_filtered.csv\n","[INFO] Reading: /content/extracted_RS_raw_data/extracted_RS_2023-06_filtered.csv\n","[INFO] Reading: /content/extracted_RS_raw_data/extracted_RS_2023-07_filtered.csv\n","[INFO] Reading: /content/extracted_RS_raw_data/extracted_RS_2023-08_filtered.csv\n","[INFO] Reading: /content/extracted_RS_raw_data/extracted_RS_2023-09_filtered.csv\n","[INFO] Reading: /content/extracted_RS_raw_data/extracted_RS_2023-10_filtered.csv\n","[INFO] Reading: /content/extracted_RS_raw_data/extracted_RS_2023-11_filtered.csv\n","[INFO] Successfully merged 12 file(s). Total rows: 20936\n","[INFO] Dropped 1 empty rows. Remaining rows: 20935\n","[DONE] Cleaned data saved to: /content/cleaned_merged_data.csv\n","[INFO] DataFrame columns: ['subreddit', 'title', 'selftext', 'created_utc', 'created_dt', 'month', 'combined_text', 'clean_text']\n","[INFO] Head of the DataFrame:\n","           subreddit                                              title  \\\n","0  ImmigrationCanada  Received the nomination letter in EE but not r...   \n","1  ImmigrationCanada  PGWP approved, Can I apply for OHIP immediatel...   \n","2  ImmigrationCanada                 OHIP on Interim work authorization   \n","\n","                                            selftext   created_utc  \\\n","0                                          [deleted]  1.669855e+09   \n","1  I have the approval letter, just waiting for t...  1.669855e+09   \n","2                                          [removed]  1.669857e+09   \n","\n","           created_dt    month  \\\n","0 2022-12-01 00:39:40  2022-12   \n","1 2022-12-01 00:42:03  2022-12   \n","2 2022-12-01 01:17:41  2022-12   \n","\n","                                       combined_text  \\\n","0  Received the nomination letter in EE but not r...   \n","1  PGWP approved, Can I apply for OHIP immediatel...   \n","2                OHIP on Interim work authorization    \n","\n","                                          clean_text  \n","0  received the nomination letter in ee but not r...  \n","1  pgwp approved, can i apply for ohip immediatel...  \n","2                 ohip on interim work authorization  \n"]}]},{"cell_type":"markdown","source":["# 新段落"],"metadata":{"id":"HlPJ7gU3SBya"}}]}